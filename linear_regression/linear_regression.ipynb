{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 线性回归的模型函数如下：\n",
    "+ $h_w(x_i)=w_0 + w_1x_1 + w_2x_2+...+w_nx_n$\n",
    "+ $h_w(x_j)=w^Tx_i=W^TX$\n",
    "## 损失函数如下：\n",
    "+ $J(W)=1/2M\\sum_{i=0}^{M}{(h_w(x_i)-y_i)}^2=1/2M(XW-y)^T(XW-Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge 回归\n",
    "+ Ridge 回归通过对系数的大小施加惩罚来解决 普通最小二乘法的一些问题。 岭系数最小化的是带罚项的残差平方和\n",
    "+ $\\min_{w}{|Xw-y|}^2 + \\alpha{|w|}^2$\n",
    "+ 其中，α≥0\n",
    "+ 是控制系数收缩量的复杂性参数： α 的值越大，收缩量越大，这样系数对共线性的鲁棒性也更强。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso回归\n",
    "+ $\\min_{w}\\frac{1}{2n_{samples}}{|Xw-y|}^2+\\alpha{|w|}$\n",
    "+ lasso 估计解决了加上罚项α||w||1的最小二乘法的最小化，其中，α 是一个常数，||w||1是参数向量的 ℓ1−norm 范数\n",
    "+ Lasso 类的实现使用了 coordinate descent （坐标下降算法）来拟合系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
